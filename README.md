<div align="center">
<h1>ConText: <br>Driving In-context Learning for Text Removal and Segmentation  </h1>

[Fei Zhang](https://scholar.google.com/citations?hl=zh-CN&user=Omrg6UkAAAAJ)<sup>1,2</sup>, &nbsp; [Pei Zhang](https://scholar.google.com/citations?user=JqhgnkwAAAAJ&hl=en)<sup>3</sup>, &nbsp; [Baosong Yang](https://scholar.google.com/citations?user=fXsHJXkAAAAJ&hl=en)<sup>3</sup>, &nbsp; [Fei Huang](https://sites.google.com/view/fei-huang)<sup>3</sup>, &nbsp; [Yanfeng Wang](https://ieeexplore.ieee.org/author/37085615187)<sup>1</sup>, &nbsp;[Ya Zhang](http://scholar.google.com/citations?user=pbjw9sMAAAAJ&hl=zh-CN)<sup>1</sup>

<sup>1</sup>[SJTU](https://www.sjtu.edu.cn/), &nbsp; <sup>2</sup>[SII](https://www.sii.edu.cn/), &nbsp; <sup>3</sup>[Qwen, Tongyi LAB](https://www.tongyi.com/)

ICML 2025, [[Paper]](https://arxiv.org/abs/2506.03799)


<br>
  
<image src="docs/teaser.png" width="540px" />
<br>

</div>

<br>

We present **ConText**, the
first exploration of establishing a visual in-context learning (V-ICL) paradigm for fine-grained text recognition tasks, including text segmentation and removal. To achieve this, we sought a single-task-targeted baseline solution based on the prevailing V-ICL frameworks, which typically regulates in-context inference as a query-label-reconstruction process. Beyond simple task-specific fine-tuning, we proposed an end-to-end in-context generalist elicited from a task-chaining prompt that explicitly chaining up tasks as one enriched demonstration, leveraging inter-task correlations to improve the in-context reasoning capabilities. A Through quantitative and
qualitative experiments, we demonstrated the grounding
effectiveness and superiority of our framework across various
in-domain and out-of-domain text recognition tasks,
outperforming both current generalists and specialists. Overall,
we hope this pioneering work will encourage further
development of V-ICL in text recognition.



### $\color{#2F6EBA}{Task-Chaining}$ 

- Removal-segmentation task connector to mutual benefit
- SOTA in both text removal and segmentation tasks
  <image src="docs/image.png" width="868px" />
- Performing human-interactive in-context reasoning without training.
  <image src="docs/icl.png" width="540px" />








## ðŸš€ Release Plan



| Process | Status |
|----------|----------|
| Inference Code Release | âœ… |
| Checkpoint Release | âœ… |
| Training Code Release |  |





## Installation
### Requirements
* Linux, CUDA>=9.2, GCC>=5.4
* PyTorch >= 1.8.1
* Other requirements
    ```bash
    pip install -r requirements.txt
    ```
* Install [detectron2](https://github.com/facebookresearch/detectron2), following the instructions in [here](https://detectron2.readthedocs.io/en/latest/tutorials/install.html). 
Or simply use the following command. (Checking the version of this and your CUDA version, in my setting, ```detectron==0.6```)
```bash
git clone https://github.com/facebookresearch/detectron2
python -m pip install -e detectron2
```



## Data

### Souces Collection


***ConText [HierText]***:

The main training data is collected from [this](https://www.modelscope.cn/datasets/Kpillow/SceneVTG-Erase), which contains the removal and segmentation annotation of ***HierText***. We only use the training set.

***ConTextV [Hiertext + TotalText + TextSeg + SCUT-EnsText]***:

The main training data is collected from [this](https://www.modelscope.cn/datasets/Kpillow/SceneVTG-Erase), which contains the removal and segmentation annotation of ***HierText***, and the train set of ***TotalText***. For the removal annotation of ***TextSeg*** or other downstream tasks, we use [ViTEraser](https://github.com/shannanyinxiang/ViTEraser) to generate the removal image for them. For the segmentation in the test set of **SCUT-EnsText** could be generated by the following simple pixel operation
```
image_bgr = cv2.imread(os.path.join(image_dir,t), cv2.IMREAD_COLOR)
rec_bgr = cv2.imread(os.path.join(images_ori,t), cv2.IMREAD_COLOR)
difference = cv2.absdiff(image_bgr, rec_bgr)
difference_gray = cv2.cvtColor(difference, cv2.COLOR_BGR2GRAY)
_, mask = cv2.threshold(difference_gray, 25, 255, cv2.THRESH_BINARY)
mask = np.repeat(np.expand_dims(mask,-1),3,axis=-1)
```

**PS**:
For all dataset, you can find the data collection process in [HISAM](https://github.com/ymy-k/Hi-SAM) and [ViTEraser](https://github.com/shannanyinxiang/ViTEraser).


### Data Json Setting
Assume you have collected all image data in `$demo_data`, which has the following structures:

```
â”œâ”€â”€ demo_data
â”‚Â Â  â”œâ”€â”€ ori
â”‚Â Â  â”œâ”€â”€ rem
â”‚Â Â  â””â”€â”€ seg
â””â”€â”€ requirements.txt
```

where `$ori`, `$seg` and `$rem` indicate the *original image*, *removal image* and *segmentation mask*.  Then you MUST generate the data json by:
```
python data/generate_json/generate_demo.py
```
Then you can get a `$data/Test/Demo_Test` file, remember to set *Test* or *Train* split, and the right data path in `generate_demo.py`.  


## Evaluation
DownLoad the corresponding checkpoint of ConText (**context.pt**) or ConTextV (**contextv.pt**):

| Model | TotalText Segmentation (fgIoU) | SCUENS Removal (PSNR)|
|----------|----------|---|
| ConText [ðŸ¤— huggingface](https://huggingface.co/ferenas/ConText)  | 78.02 | 37.67 |
| ConTextV [ðŸ¤— huggingface](https://huggingface.co/ferenas/ConText)   | 85.19 | 40.83 |


Put them in `$ckpt`, and then run the following command:
```
python context_inference.py \
    --ckpt_path ckpt/contextv.pt \
    --input_size 2048 \
    --data_used Demo_Test \
    --img_path demo_data \
```

For better performance, you can add `--slide` as you like to perform sliding window, but yielding slow inference time.

you can set `"--upper_per"` if you want to see the upper bound of the model performance since the ground truth is set to be the demonstration.

After this, you may find the visualized results of segmentation and removal in `$vis`.


## Citation

```
@article{zhang2025contextdrivingincontextlearning,
      title={ConText: Driving In-context Learning for Text Removal and Segmentation}, 
      author={Fei Zhang and Pei Zhang and Baosong Yang and Fei Huang and Yanfeng Wang and Ya Zhang},
      year={2025},
      eprint={2506.03799},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.03799}, 
}
```



## Contact ï¼ˆANY Problemï¼‰

Due to the company regulation, the training code may not be released as expected (I will try). Any **Training** Problem you can directly contact Fei Zhang (`ferenas@sjtu.edu.cn`). The main training pipeline could follow [SegGPT](https://github.com/baaivision/Painter). However, SegGPT is not released the training code, so you can refer to its predecessor Painter.



## Acknowledgement
[MAE](https://github.com/facebookresearch/mae), [BEiT](https://github.com/microsoft/unilm/tree/master/beit), [detectron2](https://github.com/facebookresearch/detectron2), [SegGPT](https://github.com/baaivision/Painter).
